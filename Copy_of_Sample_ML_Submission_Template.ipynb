{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohandhunde/Netflix_Unsupervise_project/blob/main/Copy_of_Sample_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Rohan Dhunde\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summarys -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This project involved analyzing the content available on Netflix by categorizing and clustering TV shows and movies based on attributes such as genre, cast, director, rating, country, and description. The dataset consisted of 7787 records and 11 attributes, which were preprocessed and vectorized using the TFIDF vectorizer to handle the curse of dimensionality. Two types of clusters were created using the K-Means Clustering and Agglomerative Hierarchical Clustering algorithms, and the optimal number of clusters was determined using different techniques such as the elbow method, silhouette score, and dendogram. A content-based recommender system was built using the similarity matrix obtained from cosine similarity to recommend 10 shows/movies to the user based on their preferences. The project provides insights into the Netflix dataset and helps users find similar shows/movies based on their preferences.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Links -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# Word Cloud library\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# library used for textual data preprocessing\n",
        "import string\n",
        "import unicodedata\n",
        "string.punctuation\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# library used for building recommendation system\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# library used for Clusters implementation\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "import scipy.cluster.hierarchy as shc\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set()"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace the file link with the link to your own file\n",
        "url = 'https://drive.google.com/file/d/1f0lr_EcXGUysg1dxu-BbyoUiNFmX0U6T/view?usp=sharing'\n",
        "\n",
        "# Extract the file ID from the link\n",
        "file_id = url.split('/')[-2]\n",
        "\n",
        "# Generate a download link for the file\n",
        "download_link = 'https://drive.google.com/uc?id=' + file_id\n",
        "\n",
        "# Read the CSV file into a Pandas DataFrame\n",
        "df = pd.read_csv(download_link)\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(len(df[df.duplicated()]))"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msno.matrix(df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "*   The data set contain the information about the severals movies\n",
        "*   The data set contain the non values in directio, cast, country, date_added, and also in rating columns.\n",
        "\n",
        "\n",
        "*   No method can be used to fill in missing data for each movie's specific information.\n",
        "*   \n",
        "NaN values are being replaced with empty space to prevent data loss, but this approach may not be ideal since external sources could provide missing information for some columns.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all').transpose()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset includes textual details about the content of various movies or TV shows, with a total of 7769 distinct descriptions. The most common description is \"Several females report their partners as absent,\" and there are three descriptions that have a frequency of only one. However, there are also some missing values represented by NaN."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns.tolist():\n",
        "  print(\"numbersof unique values from \",i,\"is\",df[i].nunique())\n",
        "     "
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Variables***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",df[i].nunique())"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "xjmJvbj6dkHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* After analyzing the dataset, it has been decided to focus on specific columns for modeling purposes.\n",
        "* The columns of interest are 'type', 'title', 'director', 'cast', 'country', 'rating', 'listed_in', and 'description'. These columns are believed to carry a lot of information that will be useful for modeling.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wiyPPQJGdloN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " the EDA process is critical in helping data scientists and analysts to identify and understand the significant characteristics of a dataset, such as its data quality, completeness, and variability. This knowledge can then be used to guide subsequent data processing, modeling, and decision-making activities to address specific business problems or research questions."
      ],
      "metadata": {
        "id": "-Qb5Lcnmdxpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "counts = df['type'].value_counts()\n",
        "plt.bar(counts.index, counts.values)\n",
        "plt.title(\"Count by Type\")\n",
        "plt.xlabel(\"Type\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "print(df[\"type\"].value_counts())\n",
        "\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   according to the  dataset, there are more movies then the tv shows.\n",
        "\n",
        "*   The bar plot shows that the dataset consist of 5377 movies and 2410 TV shows .\n",
        "\n",
        "*  This means that the dataset has almost two times more movies then the tv shows.\n",
        "*   The distribution of movie and TV shows in dataset is an important factor to consist to consider when analysis the data.\n",
        "\n",
        "\n",
        "*   Lets take a closet look at the charasteristics of movies and TV shows in the datase to gain more insights.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vVJkxfRsrmrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2:"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the Number of Directors for Movies and TV Shows Separately\n",
        "\n",
        "print(f\"number of director who  by directed movie : { df[df['type']=='Movie']['director'].value_counts().sum()}\")\n",
        "print(f\"number of director who  by directed TV Show : { df[df['type']=='TV Show']['director'].value_counts().sum()}\")\n",
        "   "
      ],
      "metadata": {
        "id": "OuksT0gqoRLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining fig size and axis\n",
        "fig,ax = plt.subplots(1,2, figsize=(15,5),dpi=100)\n",
        "\n",
        "# top 10 director who directed TV show\n",
        "show = df[df['type']=='TV Show']['director'].value_counts()[:10].plot(kind='barh', ax=ax[0])\n",
        "show.set_title('top 10 director who directed TV Show', size=16,color='black')\n",
        "\n",
        "# top 10 director who directed movie\n",
        "movie = df[df['type']=='Movie']['director'].value_counts()[:10].plot(kind='barh', ax=ax[1])\n",
        "movie.set_title('top 10 director who directed Movie', size=16,color='black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "     "
      ],
      "metadata": {
        "id": "X7LKEuNHrZ4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  The director Alastair Fothergill has directed three TV shows, which is the highest number of TV shows directed by any director in the dataset.\n",
        "* Raul Campos and Jan Suter have collaborated directed 18 movies, which is the highest number compared to any other director pair in the dataset. Following them are Marcus Raboy, Jay Karas, and Cathy Garcia-Molina.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 countries with the highest number movies / TV shows in the dataset\n",
        "plt.figure(figsize=(15,5),dpi=100)\n",
        "df.country.value_counts().nlargest(10).plot(kind='barh')\n",
        "plt.title('Top 10 countries with the highest number of movies / TV shows',fontsize=16,color='black')"
      ],
      "metadata": {
        "id": "oAQO7n40WoZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The highest number of movies / TV shows were based out of the US, followed by \n",
        "\n",
        " \n",
        "\n",
        "India and UK."
      ],
      "metadata": {
        "id": "MdOXwQitefdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defing fig size and axis\n",
        "fig,ax = plt.subplots(1,2, figsize=(15,5),dpi=100)\n",
        "\n",
        "# top 10 TV shows actor \n",
        "TV_shows = df[df['type']=='TV Show']['cast'].str.split(', ', expand=True).stack().reset_index(level=1, drop=True).value_counts()[:10].plot(kind='barh', ax=ax[0])\n",
        "TV_shows.set_title('Top 10 actors who appeared in Tv shows', size=16,color='black')\n",
        "\n",
        "# top 10 Movie actor \n",
        "movies = df[df['type']=='Movie']['cast'].str.split(', ', expand=True).stack().reset_index(level=1, drop=True).value_counts()[:10].plot(kind='barh', ax=ax[1])\n",
        "movies.set_title('Top 10 actors who appeared in movie', size=16,color='black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "     "
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Takahiro Sakurai, Yuki Kaji and Daisuke Ono played highest role in the TV shows.\n",
        "\n",
        "* Anupam Kher, Shahrukh Khan and Om Puri played highest number of role in the movies."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 Rating \n",
        "fig,ax = plt.subplots(1,2, figsize=(15,5),dpi=100)\n",
        "plt.suptitle('Top 10 rating given for movie and shows', size=16,color='black', y=1.01)\n",
        "\n",
        "# univariate analysis\n",
        "df['rating'].value_counts()[:10].plot(kind='barh',ax=ax[0])\n",
        "\n",
        "# bivariate analysis\n",
        "graph = sns.countplot(x=\"rating\", data=df, hue='type', order=df['rating'].value_counts().index[0:10], ax=ax[1])\n",
        "plt.xticks(rotation=90)\n",
        "for p in graph.patches:  #adding value count on the top of bar\n",
        "   graph.annotate(format(p.get_height(), '.0f'), (p.get_x(), p.get_height()))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Most of the movie and tv shows have rating of TV-MA (Mature Audiance) then followed by TV-14 (younger audiance)."
      ],
      "metadata": {
        "id": "lzrkNj6xttZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "wigR5L4Fw8La"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# duration column\n",
        "df['duration']"
      ],
      "metadata": {
        "id": "eSO7AYHnxD-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the DataFrame to keep only TV Shows\n",
        "netflix_shows = df[df['type'] == 'TV Show']\n",
        "\n",
        "# Filter the DataFrame to keep only Movies\n",
        "netflix_movies = df[df['type'] == 'Movie']\n",
        "\n",
        "# Convert the 'duration' column to integer values representing minutes\n",
        "netflix_movies['duration'] = netflix_movies['duration'].str.replace(' min', '').astype(int)\n",
        "\n",
        "# Calculate the average movie length for each release year\n",
        "avg_movie_length = netflix_movies.groupby('release_year')['duration'].mean()\n",
        "\n",
        "# Plot the average movie length over the years\n",
        "plt.figure(figsize=(15, 5), dpi=100)\n",
        "plt.plot(avg_movie_length.index, avg_movie_length.values)\n",
        "plt.title('Average movie length over the years', fontsize=16, color='black')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Length of movie in minutes')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yWPFg-q5xG8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  Netflix offers a range of movies on its platform, including those from as far back as 1942.\n",
        "\n",
        "*  Movies made in the 1940s had a relatively short duration, according to their plots.\n",
        "\n",
        "*   On average, movies made in the 1960s are the longest in length.\n",
        "\n",
        "\n",
        "\n",
        "*  The average length of movies has been decreasing steadily since the 2000s.\n",
        "\n"
      ],
      "metadata": {
        "id": "aMIXoiqrxq0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "netflix_shows = df[df['type'] == 'TV Show']\n",
        "\n",
        "# Extract duration value and unit for TV shows\n",
        "netflix_shows['duration_value'] = netflix_shows['duration'].apply(lambda x: int(x.split(' ')[0]))\n",
        "netflix_shows['duration_unit'] = netflix_shows['duration'].apply(lambda x: x.split(' ')[1])\n",
        "\n",
        "# Convert number of seasons to integer\n",
        "netflix_shows.loc[netflix_shows['duration_unit'] == 'seasons', 'duration_value'] = netflix_shows.loc[netflix_shows['duration_unit'] == 'seasons', 'duration_value'].astype(int)\n",
        "\n",
        "# Number of seasons in each TV show\n",
        "plt.figure(figsize=(15,5),dpi=100)\n",
        "p = sns.countplot(x='duration_value', data=netflix_shows)\n",
        "plt.title('Number of seasons per TV show distribution', fontsize=16, color='black')\n",
        "\n",
        "for i in p.patches:\n",
        "  p.annotate(format(i.get_height(), '.0f'), (i.get_x() + i.get_width() / 2., i.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  The TV series in the dataset have a maximum of 16 seasons, but the majority only have one season.\n",
        "*   This could suggest that many of the TV shows are relatively new and additional seasons may be in the works.\n",
        "\n",
        "*  There are very few TV shows in the dataset with more than 8 seasons.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7BrKDp2twQoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# seperating genre from listed_in columns for analysis purpose\n",
        "genres = df['listed_in'].str.split(', ', expand=True).stack().reset_index(level=1, drop=True)\n",
        "\n",
        "# top 10 genre in listed movie/show\n",
        "plt.figure(figsize=(15,5),dpi=100)\n",
        "genres = genres.value_counts()[:10].plot(kind='barh')\n",
        "plt.title('Top 10 genres',fontsize=16,color='black')\n",
        "\n"
      ],
      "metadata": {
        "id": "3llMfglrwgOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E7v1aqTAVjNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Missing Data %\n",
        "round(df.isna().mean().sort_values(ascending=False)*100,2)\n",
        "     "
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   For the missing values in the director, cast, and country attributes, the 'empty string' can be used as a replacement.\n",
        "*   The percentage of null values in the rating and date_added columns is small, and dropping these values may not significantly impact model building.\n"
      ],
      "metadata": {
        "id": "piVk7D_Tx0Mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df[['director','cast','country']] = df[['director','cast','country']].fillna(' ')\n",
        "df.dropna(axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "aMqQXemkx8-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for null values after treating them.\n",
        "df.isna().sum()\n"
      ],
      "metadata": {
        "id": "F76j4GWJx-DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "sns.boxplot(data=df,orient='h');"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Outlier handling may not be necessary for textual data as outliers are typically defined in numerical data.\n",
        "*   Data cleaning and preprocessing steps are still necessary to ensure the data is ready for model building.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aVRSEgKxyIS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "*   Outlier handling may not be necessary for textual data as outliers are typically defined in numerical data.\n",
        "*    Data cleaning and preprocessing steps are still necessary to ensure the data is ready for model building.\n",
        "\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *4. *Textual Data Preprocessing *\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Select the attributes that will be used to cluster the shows.\n",
        "*   Perform text preprocessing by removing stopwords and punctuation marks, and converting all textual data to lowercase.\n",
        "\n",
        "*   Use stemming to generate a meaningful word out of the corpus of words.\n",
        "*   Tokenize the corpus and perform word vectorization.\n",
        "\n",
        "\n",
        "*   Apply dimensionality reduction techniques to reduce the dimensionality of the dataset.\n",
        "\n",
        "\n",
        "*   Use different algorithms to cluster the movies and determine the optimal number of clusters using various techniques such as the elbow method or silhouette score.\n",
        "\n",
        "\n",
        "*   Build the optimal number of clusters and visualize the contents of each cluster using word clouds to gain insights about the characteristics of each cluster.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eOQPnc7hyqKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering Attributes"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will cluster the movie/shows on Netflix based on the following attributes:\n",
        "\n",
        "*   Director\n",
        "\n",
        "*   Cast\n",
        "\n",
        "\n",
        "*   Country\n",
        "*    Rating\n",
        "\n",
        "\n",
        "*    Listed in (genres)\n",
        "\n",
        "\n",
        "*   Description\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2S5tuFPzZbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Copying the original dataset for clustering as it does not contain any missing values to handle\n",
        "df1 = df.copy()\n",
        "     "
      ],
      "metadata": {
        "id": "Q8tVVrdezwFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating clustering_attributes column using all text column which one is used for model building purpose.\n",
        "df1['clustering_attributes'] = df1['description'] + df1['listed_in'] + df1['rating'] + df1['cast'] + df1['country'] + df1['director']"
      ],
      "metadata": {
        "id": "YeBd3Bz9zyoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.clustering_attributes[0]"
      ],
      "metadata": {
        "id": "dGD0wyGOz2gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['clustering_attributes'].head(10)"
      ],
      "metadata": {
        "id": "_isTzfeXz6Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   All the required data has been successfully consolidated into a single column.\n",
        "\n"
      ],
      "metadata": {
        "id": "_KbMnCWxz-wP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Removing non-ASCII characters:"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to remove non-ascii characters\n",
        "\n",
        "def remove_non_ascii(words):\n",
        "    \"\"\"Function to remove non-ASCII characters\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)                    # unicodedata.normalize = convert each string to NFKD form\n",
        "    return new_words                                  # encode = convert string to ASCII format\n",
        "                                                      # decode = convert resulting byte string to regular string format\n",
        "     \n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove non-ascii characters\n",
        "df1['clustering_attributes'] = remove_non_ascii(df1['clustering_attributes'])\n",
        "     "
      ],
      "metadata": {
        "id": "AF3qXRk70MXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['clustering_attributes'][0]\n",
        "     "
      ],
      "metadata": {
        "id": "fDRqZG340Mek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['clustering_attributes'].head(5)"
      ],
      "metadata": {
        "id": "MzeHNj0g0SDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Removing stopwords and lower case:"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the stop words list if it hasn't been downloaded already\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Create a set of English stop words\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# Display the stop words\n",
        "print(stop_words)\n",
        "     "
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Text Preprocessing: Removing Stopwords and Punctuation Marks, and Stemming.\n",
        "def stopwords(text):\n",
        "    '''a function for removing the stopword and lowercase the each word'''\n",
        "    text = [word.lower() for word in text.split() if word.lower() not in stop_words]\n",
        "    # joining the list of words with space separator\n",
        "    return \" \".join(text)"
      ],
      "metadata": {
        "id": "vybaejRx0bS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stop words\n",
        "df1['clustering_attributes'] = df1['clustering_attributes'].apply(stopwords)\n",
        "     "
      ],
      "metadata": {
        "id": "iSftqOfc0ck5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['clustering_attributes'][0]\n",
        "     "
      ],
      "metadata": {
        "id": "BNykHwTh0csJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The corpus has been successfully converted to lowercase and all the stopwords have been removed.\n",
        "\n"
      ],
      "metadata": {
        "id": "3zn821-N0jhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Removing Punctuation:"
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing punctuation is a common preprocessing step in natural language processing (NLP) tasks. Punctuation marks such as periods, commas, and exclamation points can add noise to the data and can sometimes be treated as separate tokens, which can impact the performance of NLP models."
      ],
      "metadata": {
        "id": "BWLDYW7X0tOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to remove punctuations\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    '''a function for removing punctuation'''\n",
        "    # replacing the punctuations with no space, which deletes the punctuation marks.\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # return the text stripped of punctuation marks\n",
        "    return text.translate(translator)\n",
        "     "
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing punctuation marks\n",
        "df1['clustering_attributes'] = df1['clustering_attributes'].apply(remove_punctuation)\n",
        "     "
      ],
      "metadata": {
        "id": "LOQBYgW001Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['clustering_attributes'][0]\n",
        "     "
      ],
      "metadata": {
        "id": "sik_f_Hv03Dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   All the punctuation marks have been successfully dropped from the corpus.\n",
        "\n"
      ],
      "metadata": {
        "id": "Vpx5M4k507Qg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stemming:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Stemming operation bundles together words with the same root. For example, the stem operation bundles \"response\" and \"respond\" into the common stem \"respon\".\n",
        "\n",
        "\n",
        "\n",
        "*  The SnowballStemmer has been used to generate a meaningful word out of the corpus of words.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7P8Jjkg41J2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an object of stemming function\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def stemming(text):    \n",
        "    '''a function which stems each word in the given text'''\n",
        "    text = [stemmer.stem(word) for word in text.split()]\n",
        "    return \" \".join(text) \n",
        "     "
      ],
      "metadata": {
        "id": "vuYKxMtj1Gbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#performing stemming operation\n",
        "df1['clustering_attributes'] = df1['clustering_attributes'].apply(stemming)\n",
        "    "
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['clustering_attributes'][0]\n",
        "     "
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Text Vectorization."
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "# # extract the tfid representation matrix of the text data\n",
        "tfid_vectorizer= TfidfVectorizer(stop_words='english', lowercase=False, max_features = 10000)  # max features = 10000 to prevent system from crashing\n",
        "tfid_matrix = tfid_vectorizer.fit_transform(df1['clustering_attributes'])        \n",
        "\n",
        "# collect the tfid matrix in numpy array\n",
        "array = tfid_matrix.toarray()  "
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Print Shape and Data Type of a NumPy Array\n",
        "print(array)\n",
        "print(f'shape of the vector : {array.shape}')\n",
        "print(f'datatype : {type(array)}')\n",
        "     "
      ],
      "metadata": {
        "id": "zcXSjcYJTtAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Dimensionality Reduction:"
      ],
      "metadata": {
        "id": "3k5h6Wg9TxmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Vectorization is the process of converting text into numerical representation"
      ],
      "metadata": {
        "id": "6kv8NM_F1ldT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Dimensionality Reduction:"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction is the process of reducing the number of features or dimensions in a dataset while preserving as much information as possible.\n",
        "\n",
        "\n",
        "* PCA (Principal Component Analysis) can be used to reduce the dimensionality of the data."
      ],
      "metadata": {
        "id": "ec75D1FDUB0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using PCA to reduce dimensionality\n",
        "pca = PCA(random_state=0)\n",
        "pca.fit(array)"
      ],
      "metadata": {
        "id": "S7UjrUGpUOL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explained variance for different number of components\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.title('PCA - Cumulative explained variance vs number of components',fontsize=16,color='black')\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance');"
      ],
      "metadata": {
        "id": "mNLu5KH_UWPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   After performing PCA, it was found that ~7600 components can explain 100% of the variance in the data.\n",
        "*  More than 80% of the variance can be explained by just 4000 components.\n",
        "\n",
        "*   Selecting the top 4000 components can help simplify the model and reduce dimensionality while still capturing more than 80% of the variance.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K-eRqsT5UXx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reducing the dimensions to 4000 using pca\n",
        "pca = PCA(n_components=4000,random_state=0)\n",
        "pca.fit(array)\n",
        "     "
      ],
      "metadata": {
        "id": "OQLIyfVkUgki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformed features\n",
        "X = pca.transform(array)\n",
        "\n",
        "# shape of transformed vectors\n",
        "X.shape"
      ],
      "metadata": {
        "id": "i8cA6sbNUkzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 :   K-Means Clustering"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   K-means clustering is a popular unsupervised machine learning algorithm that divides a dataset into a predefined number of clusters. Since it is an unsupervised algorithm, it does not rely on labeled examples to learn about the data. To determine the optimal number of clusters for the K-means algorithm, we can use the elbow curve and Silhouette score visualization techniques.\n",
        "\n"
      ],
      "metadata": {
        "id": "QI2iBxvPKtAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Elbow method to find the optimal value of k"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The elbow curve is a plot of the sum of squared distances between each point and the centroid in a cluster against the number of clusters. As the number of clusters increases, the sum of squared distances generally decreases. The \"elbow\" point on the curve represents the optimal number of clusters, beyond which the decrease in sum of squared distances is not significant.\n"
      ],
      "metadata": {
        "id": "Vk9K8nOKLUW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The Elbow Method for Determining Optimal Number of Clusters\n",
        "\n",
        "sum_of_sq_dist =[]\n",
        "for i in range(1,20):\n",
        "  # Initialize the k-means model with the current value of i\n",
        "  kmeans = KMeans(n_clusters=i,init='k-means++',random_state=0)\n",
        "  # Fit the model to the data\n",
        "  kmeans.fit(X)\n",
        "  # Compute the sum of squared errors for the model\n",
        "  sum_of_sq_dist.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the value of SSE\n",
        "number_clusters = range(1,20)\n",
        "plt.figure(figsize=(15,5),dpi=100)\n",
        "plt.plot(number_clusters,sum_of_sq_dist)\n",
        "plt.title('The Elbow Method for optimal K',fontsize=16,color='red')\n",
        "plt.xlabel('Number of clusters (K)')\n",
        "plt.ylabel('Sum of squared distances')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Both6ZgkLVx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Select the number of clusters as 10, as no drastic difference is visible after that.\n",
        "\n"
      ],
      "metadata": {
        "id": "EhL3CdJzLaGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KMeans Clustering Visualization of Data Points\n",
        "plt.figure(figsize=(10,6), dpi=150)\n",
        "\n",
        "kmeans= KMeans(n_clusters=10, init= 'k-means++', random_state=0)\n",
        "kmeans.fit(X)\n",
        "\n",
        "#predict the labels of clusters.\n",
        "label = kmeans.fit_predict(X)\n",
        "#Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        " \n",
        "#plotting the results:\n",
        "for i in unique_labels:\n",
        "    plt.scatter(X[label == i , 0] , X[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.title('KMeans Clustering Visualization',fontsize=16,color='red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YN4VrWbGLemZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Silhouette score method to find the optimal value of k"
      ],
      "metadata": {
        "id": "s3ljkOegLtF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store the silhouette score for each value of k\n",
        "silhouette_scr = []\n",
        "\n",
        "for k in range(2, 15):\n",
        "  # Initialize the k-means model with the current value of k\n",
        "  kmeans = KMeans(n_clusters=k, init='k-means++', random_state=0)\n",
        "  # Fit the model to the data\n",
        "  kmeans.fit(X)\n",
        "  # Predict the cluster labels for each point in the data\n",
        "  labels = kmeans.labels_\n",
        "  # silhouette score for the model\n",
        "  score = silhouette_score(X, labels)\n",
        "  silhouette_scr.append(score)\n",
        "  \n",
        "# Plot the Silhouette analysis\n",
        "plt.figure(figsize=(15,5),dpi=100)\n",
        "plt.plot(range(2,15), silhouette_scr)\n",
        "plt.xlabel('Number of clusters (K)') \n",
        "plt.ylabel('Silhouette score')\n",
        "plt.title('Silhouette analysis For Optimal k',fontsize=16,color='red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ck8JMPwrLyty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The highest Silhouette score is obtained for 6 clusters."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building clusters using the k-means algorithm:"
      ],
      "metadata": {
        "id": "G3eS_NzSL-NX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering the data into 6 clusters\n",
        "kmeans = KMeans(n_clusters=6, init='k-means++', random_state=0)\n",
        "kmeans.fit(X)"
      ],
      "metadata": {
        "id": "VvCqBJO8MCij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics - distortion, Silhouette score\n",
        "kmeans_distortion = kmeans.inertia_\n",
        "kmeans_silhouette_score = silhouette_score(X, kmeans.labels_)\n",
        "\n",
        "print((kmeans_distortion, kmeans_silhouette_score))"
      ],
      "metadata": {
        "id": "gyn_DQFuMPH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a kmeans cluster number attribute\n",
        "df1['kmeans_cluster'] = kmeans.labels_\n",
        "     "
      ],
      "metadata": {
        "id": "9i8VhP8VMPYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of movies and tv shows in each cluster\n",
        "plt.figure(figsize=(15,5),dpi=100)\n",
        "graph = sns.countplot(x='kmeans_cluster',data=df1, hue='type')\n",
        "plt.title('Number of movies and TV shows in each cluster',fontsize=16,color='red')\n",
        "\n",
        "# adding value count on the top of bar\n",
        "for p in graph.patches:\n",
        "   graph.annotate(format(p.get_height(), '.0f'), (p.get_x(), p.get_height()))\n",
        "     "
      ],
      "metadata": {
        "id": "CvsoEh1Y-TAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  Successfully built 6 clusters using the k-means clustering algorithm.\n",
        "\n"
      ],
      "metadata": {
        "id": "IZ1CiP11Ma0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2  : Hierarchical clustering"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The agglomerative (hierarchical) clustering algorithm is employed to construct clusters. This approach involves merging clusters that are similar, starting with each sample as a single-sample cluster, and building a hierarchy of clusters from the bottom up. To determine the optimal number of clusters, a dendrogram can be visualized when using the agglomerative (hierarchical) clustering algorithm."
      ],
      "metadata": {
        "id": "yrxB3fWYP4v9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a dendogram to decide on the number of clusters\n",
        "plt.figure(figsize=(15,5),dpi=100)  \n",
        "dend = shc.dendrogram(shc.linkage(X, method='ward'))\n",
        "plt.title('Dendrogram',fontsize=16,color='red')\n",
        "plt.xlabel('Netflix Shows')\n",
        "plt.ylabel('Distance')\n",
        "plt.axhline(y= 4.1, color='r', linestyle='--')\n",
        "     "
      ],
      "metadata": {
        "id": "9tgvFq_NP9g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "agglomerative clustering is a powerful approach for cluster construction, providing insights into the data's structure. The algorithm's hierarchical nature allows for the identification of distinct groups and relationships. By leveraging the visualization of the dendrogram, we can make informed decisions regarding the optimal number of clusters to use in our analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AQZwdjULjdlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting hierarchical clustering model\n",
        "\n",
        "hierarchical = AgglomerativeClustering(n_clusters=7, affinity='euclidean', linkage='ward')  \n",
        "hierarchical.fit_predict(X)\n",
        "     "
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a hierarchical cluster number attribute\n",
        "df1['hierarchical_cluster'] = hierarchical.labels_"
      ],
      "metadata": {
        "id": "uhcPbGtPQKUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of movies and tv shows in each cluster\n",
        "plt.figure(figsize=(15,5),dpi=100)\n",
        "graph = sns.countplot(x='hierarchical_cluster',data=df1, hue='type')\n",
        "plt.title('Number of movies and tv shows in each cluster - Hierarchical Clustering',fontsize=16,color='red')\n",
        "\n",
        "# adding value count on the top of bar\n",
        "for p in graph.patches:\n",
        "   graph.annotate(format(p.get_height(), '.0f'), (p.get_x(), p.get_height()))\n",
        "     "
      ],
      "metadata": {
        "id": "saS3Qk3bQKfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The Agglomerative (hierarchical) clustering algorithm was utilized to construct 7 clusters successfully.\n",
        "\n"
      ],
      "metadata": {
        "id": "43EOjjQeQQ_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building wordclouds for different clusters built:"
      ],
      "metadata": {
        "id": "_6zLX3c4QgtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a wordcloud for the movie descriptions\n",
        "def hierarchical_worldcloud(cluster_num):\n",
        "  comment_words = ''\n",
        "  stopwords = set(STOPWORDS)\n",
        "\n",
        "  # iterate through the csv file\n",
        "  for val in df1[df1['hierarchical_cluster']==cluster_num].description.values:\n",
        "      \n",
        "      # typecaste each val to string\n",
        "      val = str(val)\n",
        "\n",
        "      # split the value\n",
        "      tokens = val.split()\n",
        "      \n",
        "      # Converts each token into lowercase\n",
        "      for i in range(len(tokens)):\n",
        "          tokens[i] = tokens[i].lower()\n",
        "      \n",
        "      comment_words += \" \".join(tokens)+\" \"\n",
        "\n",
        "  wordcloud = WordCloud(width = 1000, height = 1000,\n",
        "                  background_color ='white',\n",
        "                  stopwords = stopwords,\n",
        "                  min_font_size = 10).generate(comment_words)\n",
        "\n",
        "\n",
        "  # plot the WordCloud image                      \n",
        "  plt.figure(figsize = (15,5), facecolor = None,dpi=100)\n",
        "  plt.imshow(wordcloud)\n",
        "  plt.axis(\"off\")\n",
        "  plt.tight_layout(pad = 0)"
      ],
      "metadata": {
        "id": "-cdkxvrpQKoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wordcloud for cluster 0\n",
        "hierarchical_worldcloud(0)"
      ],
      "metadata": {
        "id": "E7evgVlWQqBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keywords observed in cluster 1: life, love, family, world, find, friend, young, must, crime"
      ],
      "metadata": {
        "id": "GTXDvLF3Qy52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wordcloud for cluster 2\n",
        "hierarchical_worldcloud(2)"
      ],
      "metadata": {
        "id": "Jf3-D-b2Q7-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keywords observed in cluster 2: life, man, women, young, group, family, young, find, polics"
      ],
      "metadata": {
        "id": "2Bm1sfM-Q_hp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.Content based recommender system:"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Content-based recommendation systems make recommendations to users by utilizing the similarities between items. These recommendation systems suggest products or items to users based on their descriptions or features, and they determine the degree of similarity between the products by analyzing their descriptions.\n",
        "\n"
      ],
      "metadata": {
        "id": "kLMYN5roUTfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# veryfying index\n",
        "df1[['show_id', 'title', 'clustering_attributes']]\n",
        "   "
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  As shown in the above dataframe, the total number of rows present in our dataframe is 7770. However, the last index appears as 7786 due to the dropping of some rows while handling null values.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3ZeMDCyRUbOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a new df for building a recommender system\n",
        "recommender_df = df1.copy()"
      ],
      "metadata": {
        "id": "UOIeVT58UjCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reseting index\n",
        "recommender_df.reset_index(inplace=True)\n",
        "\n",
        "# checking reset index \n",
        "recommender_df[['show_id', 'title', 'clustering_attributes']]\n",
        "   "
      ],
      "metadata": {
        "id": "k-_kB2iwUk3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The index has been successfully reset, and the dataset is now ready to be used for building a content-based recommendation system.\n",
        "\n"
      ],
      "metadata": {
        "id": "0tIo0uPKUnkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping show-id and index column\n",
        "recommender_df.drop(columns=['index', 'show_id'], inplace=True)\n",
        "  "
      ],
      "metadata": {
        "id": "FPouxxgcUqtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calling out transformed array after performing PCA for dimenssionality reduction.\n",
        "X"
      ],
      "metadata": {
        "id": "bVkiNlACUtXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate cosine similarity\n",
        "similarity = cosine_similarity(X)\n",
        "similarity\n",
        "     "
      ],
      "metadata": {
        "id": "I5DWVjXTUwVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for list down top 10 recommended movie on the basis of cosine similarity score.\n",
        "def recommend(movie):\n",
        "  try:\n",
        "    '''\n",
        "    This function list down top ten movies on the basis of similarity score for that perticular movie.\n",
        "    '''\n",
        "    # Empty list\n",
        "    recommend_content = []   \n",
        "    # find out index position\n",
        "    index = recommender_df[recommender_df['title'] == movie].index[0]\n",
        "    # sorting on the basis of simliarity score, In order to find out distaces from recommended one\n",
        "    distances = sorted(list(enumerate(similarity[index])), reverse=True, key=lambda x:x[1])\n",
        "    # printing Statement\n",
        "    print(f\"If you liked '{movie}', you may also enjoy: \\n\")\n",
        "    # listing top ten recommenaded movie\n",
        "    for i in distances[1:11]:\n",
        "      recommend_content.append(df1.iloc[i[0]].title)\n",
        "    return recommend_content\n",
        "  except:\n",
        "     return 'Invalid Entry'"
      ],
      "metadata": {
        "id": "oy82COsYUzHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommend('Phir Hera Pheri')"
      ],
      "metadata": {
        "id": "6rrNTVdeU9j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommend('Welcome')"
      ],
      "metadata": {
        "id": "d--Cv01dU_tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The dataset contained 7787 records and 11 attributes. Exploratory data analysis was performed, and missing values were handled.\n",
        "\n",
        "*   Netflix has more movies than TV shows, and the number of shows on the platform is growing rapidly. Most shows are produced in the United States.\n",
        "\n",
        "\n",
        "*   The data was clustered based on attributes such as director, cast, country, genre, rating, and description. Tokenization, preprocessing, and vectorization were performed using TFIDF Vectorizer, resulting in 10,000 attributes.\n",
        "\n",
        "*   Dimensionality reduction was performed using Principal Component Analysis (PCA), with 4,000 components capturing over 80% of variance.\n",
        "\n",
        "\n",
        "*   K-Means Clustering was used to build the initial clusters, with the optimal number of clusters being 6 as determined by the elbow method and Silhouette score analysis.\n",
        "\n",
        "*   Agglomerative clustering was used to create clusters, with 7 being the optimal number determined by visualizing the dendrogram.\n",
        "\n",
        "\n",
        "*   A content-based recommender system was developed using cosine similarity on the similarity matrix. The recommender system recommends the top 10 shows based on the type of show the user has watched.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! I have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}